{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"permission-based_malware.csv\") #  read a CSV file named into a DataFrame called df\n",
    "target = \"CLASS\" # target variable or outcome variable, which represents the variable that you're trying to predict or model.\n",
    "\n",
    "# df = df.iloc[:1000, :25] # selecting the first 1000 rows and the first 25 columns of this DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10) # display the first 10 rows of the DataFrame df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # This will output a tuple with two elements: the number of rows and the number of columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # info() method in pandas DataFrame provides a concise summary of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target].unique() # extract the unique values present in the column specified by the target variable from DataFrame df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder # imports the LabelEncoder class from the scikit-learn library\n",
    "\n",
    "# encoder = LabelEncoder() # creates an instance of the LabelEncoder class, which will be used to perform the label encoding\n",
    "\n",
    "# df[target] = encoder.fit_transform(df[target]) #  label encoding transformation to the target column of the DataFrame df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # display the entire DataFrame df, including all rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target].value_counts() #  print the counts of each unique value in the target column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[: , df.columns!=target] # X will contain all the features (columns) of the DataFrame df except for the target column\n",
    "X = X.loc[: , X.columns!=\"NAME\"]\n",
    "y = df[target] # y will contain the target column, which represents the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def anova(X,y,num_of_feat): # takes three arguments: X (features), y (target variable), and num_of_feat (number of features to select).\n",
    "  fs = SelectKBest(score_func=f_classif, k=5) # select the top 5 features\n",
    "  fit = fs.fit(X,y) # Fits the feature selector to the data (X and y) to compute ANOVA F-value scores for each feature.\n",
    "\n",
    "  # Converting the ANOVA F-value scores and column names into pandas DataFrames (dfscores and dfcolumns, respectively).  \n",
    "  dfscores = pd.DataFrame(fit.scores_)\n",
    "  dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "  # Concatenate the column names and ANOVA F-value scores DataFrames along the columns axis (axis=1) to create a single DataFrame (featureScores)\n",
    "  featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "  \n",
    "  featureScores.columns = ['Best_columns','Score_ANOVA'] # Renames the columns of the DataFrame to 'Best_columns' and 'Score_ANOVA'\n",
    "\n",
    "  lyst = featureScores.nlargest(num_of_feat,'Score_ANOVA') # Selecting Top Features\n",
    "  ANOVA_feature=list(lyst['Best_columns']) # Extracts their names into a list (ANOVA_feature)\n",
    "  return ANOVA_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "\n",
    "def pearson(X,y,num_of_feat):\n",
    "  fs = SelectKBest(score_func=f_regression, k=5)\n",
    "  fit = fs.fit(X,y)\n",
    "\n",
    "  #create df for scores\n",
    "  dfscores = pd.DataFrame(fit.scores_)\n",
    "  #create df for column names\n",
    "  dfcolumns = pd.DataFrame(df.columns)\n",
    "  #concat two dataframes for better visualization\n",
    "  featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "  #naming the dataframe columns\n",
    "  featureScores.columns = ['Best_columns','Score_pearsons']\n",
    "\n",
    "  #print 10 best features\n",
    "  lyst = featureScores.nlargest(num_of_feat,'Score_pearsons')\n",
    "\n",
    "  PEARSON_feature=list(lyst['Best_columns'])\n",
    "  return PEARSON_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univarient Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def u_f(X,y,num_of_feat):\n",
    "  bestfeatures = SelectKBest(score_func=chi2, k=10) # Select the top 10 features based on the chi-squared statistic (chi2) with k=10\n",
    "  fit = bestfeatures.fit(X,y) # Fitting the Feature Selector\n",
    "\n",
    "  # Convert the chi-squared scores and column names into pandas DataFrames (dfscores and dfcolumns, respectively).\n",
    "  dfscores = pd.DataFrame(fit.scores_)\n",
    "  dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "\n",
    "  # Concat two dataframes for better visualization\n",
    "  # Concatenate the column names and chi-squared scores DataFrames along the columns axis to create a single DataFrame (featureScores).\n",
    "  featureScores = pd.concat([dfcolumns,dfscores],axis=1) \n",
    "  featureScores.columns = ['Specs','Score']  # Renames the columns of the DataFrame to 'Specs' and 'Score'\n",
    "\n",
    "\n",
    "  sorted_df = featureScores.sort_values(by=['Score'], ascending=False) # Sorts the DataFrame by the chi-squared scores in descending order.\n",
    "  Univarient_feature = sorted_df[\"Specs\"][:num_of_feat] # Selecting Top Features\n",
    "  return Univarient_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annova_list = anova(X,y,30) # Annova_list will contain the names of the top 30 features selected using ANOVA F-value.\n",
    "annova_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_list = pearson(X,y,30) #  feature selection using Pearson correlation coefficient\n",
    "pearson_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univarient_list = u_f(X,y,550) # univarient_list will contain the names of the top 30 features selected using the chi-squared scoring function\n",
    "univarient_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the features that were identified as significant through the Univariate Feature Selection\n",
    "X = df.loc[:,univarient_list] # Selects columns from the DataFrame df and stores them in the variable X\n",
    "y = df[target] # Assigns the target variable, which is typically the variable you are trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Imports the train_test_split function from scikit-learn's model_selection module.\n",
    "\n",
    "#  Splits features (X) and target variable (y) into random train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# smote = SMOTE()\n",
    "\n",
    "# X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier() # Creates an instance of the RandomForestClassifier class with default hyperparameters. \n",
    "rf.fit(X_train, y_train) # Trains (fits) the random forest model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = rf.predict(X_test) # Predicts the target labels (y_pred) for the test features (X_test) using the trained random forest classifier (rf)\n",
    "\n",
    "print(classification_report(y_test, y_pred)) # prints the classification report (provides precision, recall, F1-score, and support for each class)\n",
    "\n",
    "# prints the confusion matrixa table that shows the number of true positive, false positive, true negative, and false negative predictions.\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Creates an instance of the GradientBoostingClassifier class with specified hyperparameters.\n",
    "# n_estimators: The number of boosting stages (trees) to be used in the ensemble. max_depth: The maximum depth of the individual trees.\n",
    "gbc = GradientBoostingClassifier(n_estimators = 10, max_depth = 10) \n",
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create GradientBoostingClassifier object with specified hyperparameters\n",
    "# learning_rate: The learning rate shrinks the contribution of each tree. It's set to 0.5, which means each tree's contribution is halved.\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, max_depth=4, learning_rate=0.5)\n",
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = gbc.predict(X_train)\n",
    "\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BruteForce Tuning for Randomforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_default = RandomForestClassifier()\n",
    "rf_default.fit(X_train, y_train)\n",
    "y_pred_test=rf_default.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "print(\"Accurecy: \",accuracy_score(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "N=250\n",
    "k_range = range (1,N+1,5)\n",
    "scores={}\n",
    "scores_list = []\n",
    "scores_list_train = []\n",
    "for k in k_range:\n",
    "  classifier = RandomForestClassifier(n_estimators=k,random_state=0)\n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "  y_pred=classifier.predict(X_test)\n",
    "  test_accu = accuracy_score(y_test,y_pred)\n",
    "  scores[k] = test_accu\n",
    "  scores_list.append(test_accu)\n",
    "\n",
    "  y_pred=classifier.predict(X_train)\n",
    "  train_accu = accuracy_score(y_train,y_pred)\n",
    "  scores_list_train.append(train_accu)\n",
    "\n",
    "  print(str(k)+\"/\"+str(N)+\" round completed......................... Test Accuracy: \"+str(test_accu),\" >>>>>>> Train Accuracy: \"+str(train_accu))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (25,10))\n",
    "plt.plot(k_range,scores_list)\n",
    "plt.plot(k_range,scores_list_train)\n",
    "plt.xlabel('Value of n_estimators')\n",
    "plt.ylabel ('Testing Accuracy')\n",
    "plt.legend([\"Testing Accuracy\", \"Testing Accuracy\"], loc=0, frameon=True)\n",
    "\n",
    "\n",
    "print(\"The best n_estimators:\")\n",
    "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
    "print(best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "N=50\n",
    "k_range = range (1,N+1,5)\n",
    "scores={}\n",
    "scores_list = []\n",
    "scores_list_train = []\n",
    "for k in k_range:\n",
    "  classifier = RandomForestClassifier(max_depth=k,random_state=0)\n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "  y_pred=classifier.predict(X_test)\n",
    "  test_accu = accuracy_score(y_test,y_pred)\n",
    "  scores[k] = test_accu\n",
    "  scores_list.append(test_accu)\n",
    "\n",
    "  y_pred=classifier.predict(X_train)\n",
    "  train_accu = accuracy_score(y_train,y_pred)\n",
    "  scores_list_train.append(train_accu)\n",
    "\n",
    "\n",
    "  print(str(k)+\"/\"+str(N)+\" round completed......................... Test Accuracy: \"+str(test_accu),\" >>>>>>> Train Accuracy: \"+str(train_accu))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (25,10))\n",
    "plt.plot(k_range,scores_list)\n",
    "plt.plot(k_range,scores_list_train)\n",
    "plt.xlabel('Value of n_estimators')\n",
    "plt.ylabel ('Testing Accuracy')\n",
    "plt.legend([\"Testing Accuracy\", \"Testing Accuracy\"], loc=0, frameon=True)\n",
    "\n",
    "\n",
    "print(\"The best Depth:\")\n",
    "best_depth=list(scores.keys())[scores_list.index(max(scores_list))]\n",
    "print(best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_all = RandomForestClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
    "rf_all.fit(X_train, y_train)\n",
    "y_pred=rf_all.predict(X_test)\n",
    "print(\"Accurecy: \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_all = RandomForestClassifier(n_estimators=best_estimator,random_state=0)\n",
    "rf_all.fit(X_train, y_train)\n",
    "y_pred=rf_all.predict(X_test)\n",
    "print(\"Accurecy: \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BruteForce Tuning for CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from catboost import CatBoostClassifier\n",
    "rf_default = CatBoostClassifier()\n",
    "rf_default.fit(X_train, y_train)\n",
    "y_pred_test=rf_default.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "print(\"Accurecy: \",accuracy_score(y_test,y_pred_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tuning n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "N=250\n",
    "k_range = range (1,N+1,5)\n",
    "scores={}\n",
    "scores_list = []\n",
    "scores_list_train = []\n",
    "for k in k_range:\n",
    "  classifier = CatBoostClassifier(n_estimators=k,random_state=0)\n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "  y_pred=classifier.predict(X_test)\n",
    "  test_accu = accuracy_score(y_test,y_pred)\n",
    "  scores[k] = test_accu\n",
    "  scores_list.append(test_accu)\n",
    "\n",
    "  y_pred=classifier.predict(X_train)\n",
    "  train_accu = accuracy_score(y_train,y_pred)\n",
    "  scores_list_train.append(train_accu)\n",
    "\n",
    "  print(str(k)+\"/\"+str(N)+\" round completed......................... Test Accuracy: \"+str(test_accu),\" >>>>>>> Train Accuracy: \"+str(train_accu))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (25,10))\n",
    "plt.plot(k_range,scores_list)\n",
    "plt.plot(k_range,scores_list_train)\n",
    "plt.xlabel('Value of n_estimators')\n",
    "plt.ylabel ('Testing Accuracy')\n",
    "plt.legend([\"Testing Accuracy\", \"Testing Accuracy\"], loc=0, frameon=True)\n",
    "\n",
    "\n",
    "print(\"The best n_estimators:\")\n",
    "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
    "print(best_estimator)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "N=25\n",
    "k_range = range (1,N+1,5)\n",
    "scores={}\n",
    "scores_list = []\n",
    "scores_list_train = []\n",
    "for k in k_range:\n",
    "  classifier = CatBoostClassifier(max_depth=k,random_state=0)\n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "  y_pred=classifier.predict(X_test)\n",
    "  test_accu = accuracy_score(y_test,y_pred)\n",
    "  scores[k] = test_accu\n",
    "  scores_list.append(test_accu)\n",
    "\n",
    "  y_pred=classifier.predict(X_train)\n",
    "  train_accu = accuracy_score(y_train,y_pred)\n",
    "  scores_list_train.append(train_accu)\n",
    "\n",
    "\n",
    "  print(str(k)+\"/\"+str(N)+\" round completed......................... Test Accuracy: \"+str(test_accu),\" >>>>>>> Train Accuracy: \"+str(train_accu))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (25,10))\n",
    "plt.plot(k_range,scores_list)\n",
    "plt.plot(k_range,scores_list_train)\n",
    "plt.xlabel('Value of n_estimators')\n",
    "plt.ylabel ('Testing Accuracy')\n",
    "plt.legend([\"Testing Accuracy\", \"Testing Accuracy\"], loc=0, frameon=True)\n",
    "\n",
    "\n",
    "print(\"The best Depth:\")\n",
    "best_depth=list(scores.keys())[scores_list.index(max(scores_list))]\n",
    "print(best_depth)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "rf_all = CatBoostClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
    "rf_all.fit(X_train, y_train)\n",
    "y_pred=rf_all.predict(X_test)\n",
    "print(\"Accurecy: \",accuracy_score(y_test,y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "rf_all = CatBoostClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
    "rf_all.fit(X_train, y_train)\n",
    "y_pred=rf_all.predict(X_train)\n",
    "print(\"Accurecy: \",accuracy_score(y_train,y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BruteForce Tuning for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "rf_default = xgb.XGBClassifier() # Create an instance of XGBoost classifier\n",
    "rf_default.fit(X_train, y_train) # Train the classifier\n",
    "y_pred_test = rf_default.predict(X_test) # Predict on the test data\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Print confusion matrix, classification report, and accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tuning n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "N=250\n",
    "k_range = range (1,N+1,5)\n",
    "scores={}\n",
    "scores_list = []\n",
    "scores_list_train = []\n",
    "for k in k_range:\n",
    "  classifier = xgb.XGBClassifier(n_estimators=k,random_state=0)\n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "  y_pred=classifier.predict(X_test)\n",
    "  test_accu = accuracy_score(y_test,y_pred)\n",
    "  scores[k] = test_accu\n",
    "  scores_list.append(test_accu)\n",
    "\n",
    "  y_pred=classifier.predict(X_train)\n",
    "  train_accu = accuracy_score(y_train,y_pred)\n",
    "  scores_list_train.append(train_accu)\n",
    "\n",
    "  print(str(k)+\"/\"+str(N)+\" round completed......................... Test Accuracy: \"+str(test_accu),\" >>>>>>> Train Accuracy: \"+str(train_accu))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (25,10))\n",
    "plt.plot(k_range,scores_list)\n",
    "plt.plot(k_range,scores_list_train)\n",
    "plt.xlabel('Value of n_estimators')\n",
    "plt.ylabel ('Testing Accuracy')\n",
    "plt.legend([\"Testing Accuracy\", \"Testing Accuracy\"], loc=0, frameon=True)\n",
    "\n",
    "\n",
    "print(\"The best n_estimators:\")\n",
    "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
    "print(best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tuning max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "N=50\n",
    "k_range = range (1,N+1,5)\n",
    "scores={}\n",
    "scores_list = []\n",
    "scores_list_train = []\n",
    "for k in k_range:\n",
    "  classifier = xgb.XGBClassifier(max_depth=k,random_state=0)\n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "  y_pred=classifier.predict(X_test)\n",
    "  test_accu = accuracy_score(y_test,y_pred)\n",
    "  scores[k] = test_accu\n",
    "  scores_list.append(test_accu)\n",
    "\n",
    "  y_pred=classifier.predict(X_train)\n",
    "  train_accu = accuracy_score(y_train,y_pred)\n",
    "  scores_list_train.append(train_accu)\n",
    "\n",
    "\n",
    "  print(str(k)+\"/\"+str(N)+\" round completed......................... Test Accuracy: \"+str(test_accu),\" >>>>>>> Train Accuracy: \"+str(train_accu))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (25,10))\n",
    "plt.plot(k_range,scores_list)\n",
    "plt.plot(k_range,scores_list_train)\n",
    "plt.xlabel('Value of n_estimators')\n",
    "plt.ylabel ('Testing Accuracy')\n",
    "plt.legend([\"Testing Accuracy\", \"Testing Accuracy\"], loc=0, frameon=True)\n",
    "\n",
    "\n",
    "print(\"The best Depth:\")\n",
    "best_depth=list(scores.keys())[scores_list.index(max(scores_list))]\n",
    "print(best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_all = xgb.XGBClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
    "rf_all.fit(X_train, y_train)\n",
    "y_pred=rf_all.predict(X_test)\n",
    "print(\"Accurecy: \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_all = xgb.XGBClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
    "rf_all.fit(X_train, y_train)\n",
    "y_pred=rf_all.predict(X_train)\n",
    "print(\"Accurecy: \",accuracy_score(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BruteForce Tuning for boost-histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "rf_default = HistGradientBoostingClassifier() \n",
    "rf_default.fit(X_train, y_train) \n",
    "y_pred_test = rf_default.predict(X_test) \n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tuning max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "N=150\n",
    "k_range = range (1,N+1,5)\n",
    "scores={}\n",
    "scores_list = []\n",
    "scores_list_train = []\n",
    "for k in k_range:\n",
    "  classifier = HistGradientBoostingClassifier(max_iter=k,random_state=0)\n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "  y_pred=classifier.predict(X_test)\n",
    "  test_accu = accuracy_score(y_test,y_pred)\n",
    "  scores[k] = test_accu\n",
    "  scores_list.append(test_accu)\n",
    "\n",
    "  y_pred=classifier.predict(X_train)\n",
    "  train_accu = accuracy_score(y_train,y_pred)\n",
    "  scores_list_train.append(train_accu)\n",
    "\n",
    "  print(str(k)+\"/\"+str(N)+\" round completed......................... Test Accuracy: \"+str(test_accu),\" >>>>>>> Train Accuracy: \"+str(train_accu))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (25,10))\n",
    "plt.plot(k_range,scores_list)\n",
    "plt.plot(k_range,scores_list_train)\n",
    "plt.xlabel('Value of max_iter')\n",
    "plt.ylabel ('Testing Accuracy')\n",
    "plt.legend([\"Testing Accuracy\", \"Testing Accuracy\"], loc=0, frameon=True)\n",
    "\n",
    "\n",
    "print(\"The best max_iter:\")\n",
    "best_iter=list(scores.keys())[scores_list.index(max(scores_list))]\n",
    "print(best_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tuning max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "N=50\n",
    "k_range = range (1,N+1,5)\n",
    "scores={}\n",
    "scores_list = []\n",
    "scores_list_train = []\n",
    "for k in k_range:\n",
    "  classifier = HistGradientBoostingClassifier(max_depth=k,random_state=0)\n",
    "  classifier.fit(X_train, y_train)\n",
    "\n",
    "  y_pred=classifier.predict(X_test)\n",
    "  test_accu = accuracy_score(y_test,y_pred)\n",
    "  scores[k] = test_accu\n",
    "  scores_list.append(test_accu)\n",
    "\n",
    "  y_pred=classifier.predict(X_train)\n",
    "  train_accu = accuracy_score(y_train,y_pred)\n",
    "  scores_list_train.append(train_accu)\n",
    "\n",
    "\n",
    "  print(str(k)+\"/\"+str(N)+\" round completed......................... Test Accuracy: \"+str(test_accu),\" >>>>>>> Train Accuracy: \"+str(train_accu))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (25,10))\n",
    "plt.plot(k_range,scores_list)\n",
    "plt.plot(k_range,scores_list_train)\n",
    "plt.xlabel('Value of n_estimators')\n",
    "plt.ylabel ('Testing Accuracy')\n",
    "plt.legend([\"Testing Accuracy\", \"Testing Accuracy\"], loc=0, frameon=True)\n",
    "\n",
    "\n",
    "print(\"The best Depth:\")\n",
    "best_depth=list(scores.keys())[scores_list.index(max(scores_list))]\n",
    "print(best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_all = HistGradientBoostingClassifier(max_iter=best_estimator,max_depth=best_depth,random_state=0)\n",
    "rf_all.fit(X_train, y_train)\n",
    "y_pred=rf_all.predict(X_test)\n",
    "print(\"Accurecy: \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_all = HistGradientBoostingClassifier(max_iter=best_estimator,max_depth=best_depth,random_state=0)\n",
    "rf_all.fit(X_train, y_train)\n",
    "y_pred=rf_all.predict(X_train)\n",
    "print(\"Accurecy: \",accuracy_score(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3520887,
     "sourceId": 6140055,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
