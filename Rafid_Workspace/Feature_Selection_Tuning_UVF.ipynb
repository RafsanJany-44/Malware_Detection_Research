{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "093691bc",
   "metadata": {},
   "source": [
    "# Step 2.1: Find out with how many features univarient produces the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d802187-e16b-46e0-911b-6736f4cf7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def u_f(X,y,num_of_feat):\n",
    "  bestfeatures = SelectKBest(score_func=chi2, k=10) # Select the top 10 features based on the chi-squared statistic (chi2) with k=10\n",
    "  fit = bestfeatures.fit(X,y) # Fitting the Feature Selector\n",
    "\n",
    "  # Convert the chi-squared scores and column names into pandas DataFrames (dfscores and dfcolumns, respectively).\n",
    "  dfscores = pd.DataFrame(fit.scores_)\n",
    "  dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "\n",
    "  # Concat two dataframes for better visualization\n",
    "  # Concatenate the column names and chi-squared scores DataFrames along the columns axis to create a single DataFrame (featureScores).\n",
    "  featureScores = pd.concat([dfcolumns,dfscores],axis=1) \n",
    "  featureScores.columns = ['Specs','Score']  # Renames the columns of the DataFrame to 'Specs' and 'Score'\n",
    "\n",
    "\n",
    "  sorted_df = featureScores.sort_values(by=['Score'], ascending=False) # Sorts the DataFrame by the chi-squared scores in descending order.\n",
    "  Univarient_feature = sorted_df[\"Specs\"][:num_of_feat] # Selecting Top Features\n",
    "  return Univarient_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8bffade-5968-4be1-acd9-76ed1b742f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"permission-based_malware.csv\") #  read a CSV file named into a DataFrame called df\n",
    "target = \"CLASS\" # target variable or outcome variable, which represents the variable that you're trying to predict or model.\n",
    "\n",
    "X = df.loc[: , df.columns!=target] # X will contain all the features (columns) of the DataFrame df except for the target column\n",
    "X = X.loc[: , X.columns!=\"NAME\"]\n",
    "y = df[target] # y will contain the target column, which represents the target variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e647f3-e003-461b-9399-e3d57f144913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of column:  50\n",
      "Testing Accurecy:  0.8974358974358975\n",
      "Training Accurecy:  0.9218415417558886\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  100\n",
      "Testing Accurecy:  0.9145299145299145\n",
      "Training Accurecy:  0.9443254817987152\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  150\n",
      "Testing Accurecy:  0.9145299145299145\n",
      "Training Accurecy:  0.9421841541755889\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  200\n",
      "Testing Accurecy:  0.9145299145299145\n",
      "Training Accurecy:  0.9411134903640257\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  250\n",
      "Testing Accurecy:  0.9145299145299145\n",
      "Training Accurecy:  0.9453961456102784\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  300\n",
      "Testing Accurecy:  0.9316239316239316\n",
      "Training Accurecy:  0.9518201284796574\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  350\n",
      "Testing Accurecy:  0.9188034188034188\n",
      "Training Accurecy:  0.9528907922912205\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  400\n",
      "Testing Accurecy:  0.9188034188034188\n",
      "Training Accurecy:  0.9528907922912205\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  450\n",
      "Testing Accurecy:  0.9188034188034188\n",
      "Training Accurecy:  0.9528907922912205\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  500\n",
      "Testing Accurecy:  0.9188034188034188\n",
      "Training Accurecy:  0.9528907922912205\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  550\n",
      "Testing Accurecy:  0.9188034188034188\n",
      "Training Accurecy:  0.9528907922912205\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  600\n",
      "Testing Accurecy:  0.9188034188034188\n",
      "Training Accurecy:  0.9528907922912205\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  650\n",
      "Testing Accurecy:  0.9188034188034188\n",
      "Training Accurecy:  0.9528907922912205\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  700\n",
      "Testing Accurecy:  0.9316239316239316\n",
      "Training Accurecy:  0.9539614561027837\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  750\n",
      "Testing Accurecy:  0.9316239316239316\n",
      "Training Accurecy:  0.9646680942184154\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  800\n",
      "Testing Accurecy:  0.9316239316239316\n",
      "Training Accurecy:  0.9646680942184154\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  850\n",
      "Testing Accurecy:  0.9316239316239316\n",
      "Training Accurecy:  0.9646680942184154\n",
      "___________________________________________________________________________________________\n",
      "Number of column:  900\n",
      "Testing Accurecy:  0.9316239316239316\n",
      "Training Accurecy:  0.9646680942184154\n",
      "___________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i in range(50, len(X.columns), 50):\n",
    "    print(\"Number of column: \",i)\n",
    "    univarient_list = u_f(X,y,i) # univarient_list will contain the names of the top features selected using the chi-squared scoring function\n",
    "    \n",
    "    X_ = df.loc[:,univarient_list] # Selects columns from the DataFrame df and stores them in the variable X\n",
    "    y_ = df[target] # Assigns the target variable, which is typically the variable you are trying to predict\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split # Imports the train_test_split function from scikit-learn's model_selection module.\n",
    "    \n",
    "    #  Splits features (X) and target variable (y) into random train and test subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state = 0)\n",
    "    \n",
    "    hist = HistGradientBoostingClassifier(max_iter=51,max_depth=16,random_state=0)\n",
    "    hist.fit(X_train, y_train)\n",
    "    y_pred = hist.predict(X_test)\n",
    "    print(\"Testing Accurecy: \",accuracy_score(y_test,y_pred))\n",
    "    \n",
    "    hist.fit(X_train, y_train)\n",
    "    y_pred = hist.predict(X_train)\n",
    "    print(\"Training Accurecy: \",accuracy_score(y_train,y_pred))\n",
    "    print(\"___________________________________________________________________________________________\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
